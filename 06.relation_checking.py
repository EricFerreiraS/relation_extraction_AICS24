import numpy as np
import pandas as pd
from kgtk.functions import kgtk, kypher
import networkx as nx
from pyvis.network import Network
from lemminflect import getLemma, getAllLemmas
import torch
import ast
import spacy
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import gensim.downloader
import nltk
from nltk.corpus import wordnet
import matplotlib.pyplot as plt
import glob
import mapply
import logging

pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)

from tqdm import tqdm
import os
path='NetDissect-Lite/' #concepts generated by NetDissect

import settings

logger = logging.getLogger(__name__)
logging.basicConfig(filename=f'relation_extraction/06_{settings.DATASET_TRANS}_{settings.MODEL}.log', encoding='utf-8', level=logging.DEBUG)

os.environ['GENSIM_DATA_DIR'] = 'word_models'

print('Downloading the models')
glove_vectors = gensim.downloader.load('glove-wiki-gigaword-300')
google_vectors = gensim.downloader.load('word2vec-google-news-300')
nlp = spacy.load("en_core_web_lg")
nlp.to_disk('word_models')

nltk.data.path.append("word_models/")
nltk.download('wordnet', download_dir='word_models')
nltk.download('punkt', download_dir='word_models')

global_features = pd.read_csv(path+f'result/global_positive_unique_features_{settings.DATASET_TRANS}_{settings.MODEL}_{settings.DATASET}.csv')
local_features = pd.read_csv(path+f'result/local_positive_unique_features_{settings.DATASET_TRANS}_{settings.MODEL}_{settings.DATASET}.csv')

if settings.DATASET_TRANS == 'cifar10':
    global_features['class'] = global_features['class'].str.replace('_','')
    local_features['class'] = local_features['class'].str.replace('_','')

print('Working with the global features')
logger.info('Working with the global features')
gf_10 = global_features.groupby('class').head(10).sort_values(['class','unit_rank'])
gf_10_list = pd.DataFrame(gf_10.groupby('class')['label'].apply(list).reset_index())

relation=['/r/PartOf','/r/MadeOf','/r/IsA','/r/HasProperty','/r/HasA','mw:MayHaveProperty','/r/InstanceOf','/r/FormOf',
 '/r/UsedFor','/r/Synonym','/r/SymbolOf','/r/DefinedAs','/r/MannerOf','/r/SimilarTo']

def return_relations(name,folder='classes',kind='class',c=None):
        if folder == 'classes' and kind == 'class':
            df = kgtk(f"""query -i relation_extraction/graphs/{folder}/{settings.DATASET_TRANS}_{settings.MODEL}/{kind}_{name}.tsv""")
        elif folder == 'images' and kind == 'class':
            csv_files = glob.glob(f'relation_extraction/graphs/{folder}/{settings.DATASET_TRANS}_{settings.MODEL}/{name}/*.tsv')
            df_list = (pd.read_csv(file) for file in csv_files)
            df_ = pd.concat(df_list, ignore_index=True)
            df_.to_csv(f'relation_extraction/graphs/{folder}/{settings.DATASET_TRANS}_{settings.MODEL}/{name}.tsv')
            df = kgtk(f"""query -i relation_extraction/graphs/{folder}/{settings.DATASET_TRANS}_{settings.MODEL}/{name}.tsv""")
        else:
            df = kgtk(f"""query -i relation_extraction/graphs/{folder}/{settings.DATASET_TRANS}_{settings.MODEL}/{c}/{name}.tsv""")
        return df

classes = list(gf_10_list['class'].unique())
dfs = []
for c in tqdm(classes):
    d = return_relations(c)
    d['class'] = c
    dfs.append(d)

relations_global = pd.concat(dfs)
relations_global = relations_global.rename(columns={'node1;label':'node1_label','relation;label':'relation_label','node2;label':'node2_label'})

fig, ax = plt.subplots(figsize=(20, 20))
relations_global.drop_duplicates(['node1_label','relation_label','node2_label'], keep='first')['class'].value_counts().sort_index().plot(kind='bar').get_figure().savefig(f'relation_extraction/{settings.DATASET_TRANS}_{settings.MODEL}/relations_global.png')

relations_global.drop_duplicates(['node1_label','relation_label','node2_label'], keep='first')['class'].value_counts().mean()

def str_to_frozenset(s):
    return frozenset(ast.literal_eval((s.split('frozenset')[-1][1:-1].replace('{','(').replace('}',')'))))

df_rate = pd.read_csv(f'relation_extraction/{settings.DATASET_TRANS}_{settings.MODEL}/VG_netdissect_top10_selected_rate.csv',converters={'set': str_to_frozenset})

df_selected = pd.read_csv(f'relation_extraction/{settings.DATASET_TRANS}_{settings.MODEL}/VG_netdissect_top10_selected.csv')

def are_synonyms_spacy(str1, str2, threshold=0.9):
    # Parse the input strings with spaCy
    doc1 = nlp(str1)
    doc2 = nlp(str2)

    # Get the word vectors for each word in the input strings
    vecs1 = [token.vector for token in doc1]
    vecs2 = [token.vector for token in doc2]

    # Compute the cosine similarity between the mean vectors for each set of words
    mean_vec1 = np.mean(vecs1, axis=0)
    mean_vec2 = np.mean(vecs2, axis=0)

    #sim = mean_vec1.dot(mean_vec2) / (np.linalg.norm(mean_vec1) * np.linalg.norm(mean_vec2))
    sim = cosine_similarity(mean_vec1.reshape(1,-1),mean_vec2.reshape(1,-1))[0][0]
    # If the cosine similarity is above a threshold, consider the strings synonyms
    if sim >= threshold:
        return True
    else:
        return False

def are_synonyms_glove(str1,str2, threshold=0.9):
    tokens1 = str1.lower().split()
    tokens2 = str2.lower().split()
    sim = glove_vectors.n_similarity(tokens1, tokens2)
    if sim >= threshold:
        return True
    else:
        return False

def are_synonyms_wordnet(str1, str2):

    # Define a function to check if two words are synonyms using WordNet
    def is_synonym(word1, word2):
        synsets1 = wordnet.synsets(word1)
        synsets2 = wordnet.synsets(word2)

        for synset1 in synsets1:
            for synset2 in synsets2:
                if synset1.wup_similarity(synset2) is not None and synset1.wup_similarity(synset2) > 0.9:
                    return True
        
        return False

    # Define a function to check if two phrases are synonymous using WordNet
    def is_sentence_synonym(phrase1, phrase2):
        tokens1 = nltk.word_tokenize(phrase1)
        tokens2 = nltk.word_tokenize(phrase2)

        # Check if each word in the first phrase is synonymous with any word in the second phrase
        for token1 in tokens1:
            for token2 in tokens2:
                if is_synonym(token1, token2):
                    return True
        
        # Check if each word in the second phrase is synonymous with any word in the first phrase
        for token2 in tokens2:
            for token1 in tokens1:
                if is_synonym(token2, token1):
                    return True
        
        return False


    return is_sentence_synonym(str1, str2)

def are_synonyms_google(str1,str2, threshold=0.9):
    tokens1 = str1.lower().split()
    tokens2 = str2.lower().split()
    sim = google_vectors.n_similarity(tokens1, tokens2)
    if sim >= threshold:
        return True
    else:
        return False

def check_similarity(str1,str2):
    result1 = are_synonyms_wordnet(str1,str2)
    result2 = are_synonyms_spacy(str1,str2)
    result3 = are_synonyms_glove(str1,str2)
    result4 = are_synonyms_google(str1,str2)

    if result1 + result2 + result3 + result4 > 0:
        return True
    else:
        return False

def check_relationships(obj1,rel,obj2,df=df_rate):
    rel_list = rel.split('|')
    rel_dict = {}
    count_not = 0
    count_not_all = 0
    count_tot = 0
    for i in tqdm(rel_list):
        rate = df[df['set']==frozenset([obj1,i,obj2])]
        if rate['rate'].values.size == 0: ##didn't find the full connection, trying to get by similarity between the relations
            count_not+=1
            relations = df_selected[((df_selected['object1']==obj1) & (df_selected['object2']==obj2)) | ((df_selected['object1']==obj2) & (df_selected['object2']==obj1))]['relation'].values
            for r in set(relations):
                if check_similarity(i,r):
                    if df[df['set']==frozenset([obj1,r,obj2])]['rate'].values.size == 0:
                        count_not_all+=1
                        rel_dict[r] = 0
                    else: 
                        rel_dict[r] = df[df['set']==frozenset([obj1,r,obj2])]['rate'].values[0]
        else:
            rel_dict[i] = rate['rate'].values[0]
        count_tot+=1
    logger.info(f'Total checked: {count_tot}')
    logger.info(f'Total not find: {count_not}')
    logger.info(f'Total not find at all: {count_not_all}')
    return sum(rel_dict.values())/len(rel_dict) if len(rel_dict) > 0 else 0

mapply.init(
    n_workers=40,
    chunk_size=20,
    progressbar=True
)

relations_global['rate'] = relations_global.mapply(lambda x: check_relationships(x.node1_label,x.relation_label,x.node2_label), axis=1)

print('% relationships from KG that were learned in the NetDissect: ' + str(round((relations_global[relations_global['rate']>0].shape[0]/relations_global.shape[0])*100)) + '%')

logger.info('% relationships from KG that were learned in the NetDissect: ' + str(round((relations_global[relations_global['rate']>0].shape[0]/relations_global.shape[0])*100)) + '%')

relations_global.to_csv(f'relation_extraction/{settings.DATASET_TRANS}_{settings.MODEL}/relations_global_all.csv')

relations_gb = relations_global[relations_global['rate']>0]

# Looking the cases that have more than once (MTO) image identified by Netdissect

df_rate_mto = pd.read_csv(f'relation_extraction/{settings.DATASET_TRANS}_{settings.MODEL}/VG_netdissect_top10_selected_rate_mto.csv',converters={'set': str_to_frozenset})

relations_global_mto = relations_global.copy()

mapply.init(
    n_workers=40,
    chunk_size=20,
    #max_chunks_per_worker=1000,
    progressbar=True
)
relations_global_mto['rate'] = relations_global_mto.mapply(lambda x: check_relationships(x.node1_label,x.relation_label,x.node2_label,df_rate_mto), axis=1)

print('% relationships from KG that were learned in the NetDissect (MTO): ' + str(round((relations_global_mto[relations_global_mto['rate']>0].shape[0]/relations_global_mto.shape[0])*100)) + '%')

logger.info('% relationships from KG that were learned in the NetDissect (MTO): ' + str(round((relations_global_mto[relations_global_mto['rate']>0].shape[0]/relations_global_mto.shape[0])*100)) + '%'
            )
relations_global_mto.to_csv(f'relation_extraction/{settings.DATASET_TRANS}_{settings.MODEL}/relations_global_mto_all.csv')

relations_gb_mto = relations_global_mto[relations_global_mto['rate']>0]

relations_gb_mto.to_csv(f'relation_extraction/{settings.DATASET_TRANS}_{settings.MODEL}/relations_global_mto.csv')

relations_gb.to_csv(f'relation_extraction/{settings.DATASET_TRANS}_{settings.MODEL}/relations_global.csv')

# #### Looking for local features
print('Working with local features, based on the classes')
logger.info('Working with local features, based on the classes')
## Getting the top 10 local features for each class
lf_10 = local_features.groupby('name').head(10).sort_values(['class','unit_rank'])
lf_10_list = pd.DataFrame(lf_10.groupby(['name','class'])['label'].apply(list).reset_index())
lf_10_list_class = pd.DataFrame(lf_10.groupby(['class'])['label'].apply(set).reset_index())

print('Working with the local features, based on the images')
logger.info('Working with the local features, based on the images')
# Trying for each image, using apply with paralelism 
files = list(lf_10_list['name'])

dict_files = {}
for i in files:
    dict_files[i] = lf_10_list[lf_10_list['name']==i]['class'].values[0]

dfs_l = []
for k,v in dict_files.items():
    d = return_relations(k,'images','local',v)
    d['class'] = lf_10_list[lf_10_list['name']==k]['class'].values[0]
    d['name'] = k
    dfs_l.append(d)

relations_local_img = pd.concat(dfs_l)
relations_local_img = relations_local_img.rename(columns={'node1;label':'node1_label','relation;label':'relation_label','node2;label':'node2_label'})

relations_local_img.drop_duplicates(['node1_label','relation_label','node2_label'], keep='first')['class'].value_counts().sort_index().plot(kind='bar').get_figure().savefig(f'relation_extraction/{settings.DATASET_TRANS}_{settings.MODEL}/relations_local_img.png')

relations_local_img.drop_duplicates(['node1_label','relation_label','node2_label'], keep='first')['class'].value_counts().mean()

relations_local_img = relations_local_img.reset_index(drop=True)

mapply.init(
    n_workers=40,
    chunk_size=20,
    #max_chunks_per_worker=1000,
    progressbar=True
)

relations_local_img['rate'] = relations_local_img.mapply(lambda x: check_relationships(x.node1_label,x.relation_label,x.node2_label), axis=1)

print('% relationships from KG that were learned in the NetDissect: ' + str(round((relations_local_img[relations_local_img['rate']>0].shape[0]/relations_local_img.shape[0])*100)) + '%')

logger.info('% relationships from KG that were learned in the NetDissect: ' + str(round((relations_local_img[relations_local_img['rate']>0].shape[0]/relations_local_img.shape[0])*100)) + '%')

relations_local_img.to_csv(f'relation_extraction/{settings.DATASET_TRANS}_{settings.MODEL}/relations_local_img_all.csv')

relations_lc_img = relations_local_img[relations_local_img['rate']>0]

relations_lc_img.to_csv(f'relation_extraction/{settings.DATASET_TRANS}_{settings.MODEL}/relations_local_img.csv')

mapply.init(
    n_workers=40,
    chunk_size=20,
    #max_chunks_per_worker=1000,
    progressbar=True
)

relations_local_img_mto = relations_local_img.copy()
relations_local_img_mto['rate'] = relations_local_img_mto.mapply(lambda x: check_relationships(x.node1_label,x.relation_label,x.node2_label,df_rate_mto), axis=1)

print('% relationships from KG that were learned in the NetDissect (MTO): ' + str(round((relations_local_img_mto[relations_local_img_mto['rate']>0].shape[0]/relations_local_img_mto.shape[0])*100)) + '%')

logger.info('% relationships from KG that were learned in the NetDissect (MTO): ' + str(round((relations_local_img_mto[relations_local_img_mto['rate']>0].shape[0]/relations_local_img_mto.shape[0])*100)) + '%')

relations_local_img_mto.to_csv(f'relation_extraction/{settings.DATASET_TRANS}_{settings.MODEL}/relations_local_img_mto_all.csv')

relations_lc_img_mto = relations_local_img_mto[relations_local_img_mto['rate']>0]

relations_lc_img_mto.to_csv(f'relation_extraction/{settings.DATASET_TRANS}_{settings.MODEL}/relations_local_img_mto.csv')